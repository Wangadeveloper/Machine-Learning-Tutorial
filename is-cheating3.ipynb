{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401eb156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:16:36.876930Z",
     "iopub.status.busy": "2025-10-12T08:16:36.876521Z",
     "iopub.status.idle": "2025-10-12T08:16:38.419074Z",
     "shell.execute_reply": "2025-10-12T08:16:38.418137Z"
    },
    "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
    "outputId": "9495f150-9d79-4910-d6e7-6c0d9aae4a41",
    "papermill": {
     "duration": 1.550772,
     "end_time": "2025-10-12T08:16:38.420494",
     "exception": false,
     "start_time": "2025-10-12T08:16:36.869722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mercor-ai-detection/sample_submission.csv\n",
      "/kaggle/input/mercor-ai-detection/train.csv\n",
      "/kaggle/input/mercor-ai-detection/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f45706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:16:38.430977Z",
     "iopub.status.busy": "2025-10-12T08:16:38.430408Z",
     "iopub.status.idle": "2025-10-12T08:16:38.496161Z",
     "shell.execute_reply": "2025-10-12T08:16:38.495278Z"
    },
    "papermill": {
     "duration": 0.071919,
     "end_time": "2025-10-12T08:16:38.497268",
     "exception": false,
     "start_time": "2025-10-12T08:16:38.425349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merging:\n",
      "                                id  \\\n",
      "0    form_r_d5TpupthGXvfy58LDIbkqp   \n",
      "1  form_r_AAABmK2rZBBbtfdHxzhBY4C4   \n",
      "2    form_r_Cz6IJIWUj1B7pdyO6zPhGF   \n",
      "3  form_r_AAABmKyCfdO6NLv8eqBPCYD_   \n",
      "4    form_r_WS8I1NJpwvIcFL3Xv0Qhuc   \n",
      "\n",
      "                                               topic  \\\n",
      "0  A girl wakes from a dream and she is not sure ...   \n",
      "1  A journalistic review piece about the top 6 ai...   \n",
      "2  The influence of fictional universities in cam...   \n",
      "3                           Why do girls love horses   \n",
      "4  Every year, a remote mountain town elects a ne...   \n",
      "\n",
      "                                              answer  is_cheating  \n",
      "0  My eyes flew open, and the air around me feels...            1  \n",
      "1  Robot Butlers in the year of 2025. What are th...            0  \n",
      "2  In recent years, apparel featuring the names a...            1  \n",
      "3  The moment before I hit the dirt, I thought we...            0  \n",
      "4  In the valley of Eldermist, were the mountains...            1  \n",
      "After merging:\n",
      "                                                text  is_cheating\n",
      "0  A girl wakes from a dream and she is not sure ...            1\n",
      "1  A journalistic review piece about the top 6 ai...            0\n",
      "2  The influence of fictional universities in cam...            1\n",
      "3  Why do girls love horses The moment before I h...            0\n",
      "4  Every year, a remote mountain town elects a ne...            1\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ðŸ§¹ STEP 1: Load and preprocess dataset for AI-text detection\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "\n",
    "# Adjust these paths as needed for your Kaggle environment\n",
    "train_path = \"/kaggle/input/mercor-ai-detection/train.csv\"\n",
    "test_path = \"/kaggle/input/mercor-ai-detection/test.csv\"\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Before merging:\")\n",
    "print(train.head())\n",
    "\n",
    "# Combine topic and answer into a single text column\n",
    "train[\"text\"] = train[\"topic\"].astype(str) + \" \" + train[\"answer\"].astype(str)\n",
    "test[\"text\"] = test[\"topic\"].astype(str) + \" \" + test[\"answer\"].astype(str)\n",
    "\n",
    "# Keep only necessary columns\n",
    "train = train[[\"text\", \"is_cheating\"]]\n",
    "test = test[[\"id\", \"text\"]]\n",
    "\n",
    "print(\"After merging:\")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d53e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:16:38.506748Z",
     "iopub.status.busy": "2025-10-12T08:16:38.506548Z",
     "iopub.status.idle": "2025-10-12T08:16:38.516238Z",
     "shell.execute_reply": "2025-10-12T08:16:38.515346Z"
    },
    "papermill": {
     "duration": 0.015697,
     "end_time": "2025-10-12T08:16:38.517366",
     "exception": false,
     "start_time": "2025-10-12T08:16:38.501669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_cheating\n",
      "1    147\n",
      "0    122\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train[\"is_cheating\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b37ffdb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:16:38.526785Z",
     "iopub.status.busy": "2025-10-12T08:16:38.526314Z",
     "iopub.status.idle": "2025-10-12T08:16:38.555852Z",
     "shell.execute_reply": "2025-10-12T08:16:38.554969Z"
    },
    "papermill": {
     "duration": 0.035597,
     "end_time": "2025-10-12T08:16:38.556992",
     "exception": false,
     "start_time": "2025-10-12T08:16:38.521395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 215\n",
      "Validation samples: 54\n"
     ]
    }
   ],
   "source": [
    "def random_split(df, train_frac=0.8):\n",
    "    \"\"\"\n",
    "    Randomly splits a DataFrame into training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataset.\n",
    "        train_frac (float): Fraction of data to use for training (default 0.8).\n",
    "        \n",
    "    Returns:\n",
    "        train_df (pd.DataFrame), validation_df (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    # Shuffle the DataFrame\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Compute split index\n",
    "    train_end = int(len(df) * train_frac)\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:]\n",
    "\n",
    "    return train_df, validation_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "train_df, validation_df = random_split(train, train_frac=0.8)\n",
    "\n",
    "# Optionally save to disk\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "validation_df.to_csv(\"validation.csv\", index=False)\n",
    "test.to_csv(\"test.csv\" ,index=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(validation_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7884454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:16:38.566478Z",
     "iopub.status.busy": "2025-10-12T08:16:38.565766Z",
     "iopub.status.idle": "2025-10-12T08:16:39.500495Z",
     "shell.execute_reply": "2025-10-12T08:16:39.499780Z"
    },
    "papermill": {
     "duration": 0.940703,
     "end_time": "2025-10-12T08:16:39.501758",
     "exception": false,
     "start_time": "2025-10-12T08:16:38.561055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39ee3a54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:16:39.511838Z",
     "iopub.status.busy": "2025-10-12T08:16:39.511580Z",
     "iopub.status.idle": "2025-10-12T08:16:43.758916Z",
     "shell.execute_reply": "2025-10-12T08:16:43.758124Z"
    },
    "papermill": {
     "duration": 4.253677,
     "end_time": "2025-10-12T08:16:43.760180",
     "exception": false,
     "start_time": "2025-10-12T08:16:39.506503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token length (train set): 1024\n",
      "Train samples: 215 | Validation samples: 54\n",
      "Sample token IDs: tensor([ 9704, 13629,   287,   262,  9552,   995, 22137,   287,   262,  9552,\n",
      "          995,   318,   257,   845,  3024,  7243,   287,   284, 12545,   640])\n",
      "Sample label: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "class AIDetectionDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer=None, max_length=1024, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Initialize tokenizer (use GPT-2 encoding if none provided)\n",
    "        self.tokenizer = tokenizer or tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "        # Pre-tokenize all texts\n",
    "        self.encoded_texts = [\n",
    "            self.tokenizer.encode(str(text)) for text in self.data[\"text\"]\n",
    "        ]\n",
    "\n",
    "        # Determine or set max_length\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "\n",
    "        # Truncate and pad sequences\n",
    "        self.encoded_texts = [\n",
    "            (encoded[:self.max_length] + [pad_token_id] * max(0, self.max_length - len(encoded)))\n",
    "            for encoded in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"is_cheating\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        \"\"\"Find the maximum tokenized sequence length.\"\"\"\n",
    "        return max(len(encoded) for encoded in self.encoded_texts)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "train_dataset = AIDetectionDataset(\"train.csv\", tokenizer=tokenizer)\n",
    "val_dataset = AIDetectionDataset(\"validation.csv\", tokenizer=tokenizer)\n",
    "test_dataset = AIDetectionDataset(\"test.csv\", tokenizer=tokenizer)\n",
    "\n",
    "print(f\"Max token length (train set): {train_dataset.max_length}\")\n",
    "print(f\"Train samples: {len(train_dataset)} | Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Example: get one batch item\n",
    "sample_input, sample_label = train_dataset[0]\n",
    "print(\"Sample token IDs:\", sample_input[:20])\n",
    "print(\"Sample label:\", sample_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e2ac11c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:16:43.770333Z",
     "iopub.status.busy": "2025-10-12T08:16:43.770105Z",
     "iopub.status.idle": "2025-10-12T08:16:43.780504Z",
     "shell.execute_reply": "2025-10-12T08:16:43.779662Z"
    },
    "papermill": {
     "duration": 0.016873,
     "end_time": "2025-10-12T08:16:43.781602",
     "exception": false,
     "start_time": "2025-10-12T08:16:43.764729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 26 | Val batches: 7 | Test batches: 33\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)} | Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be211c0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:16:43.791544Z",
     "iopub.status.busy": "2025-10-12T08:16:43.790985Z",
     "iopub.status.idle": "2025-10-12T08:16:43.848019Z",
     "shell.execute_reply": "2025-10-12T08:16:43.847313Z"
    },
    "papermill": {
     "duration": 0.063231,
     "end_time": "2025-10-12T08:16:43.849152",
     "exception": false,
     "start_time": "2025-10-12T08:16:43.785921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 1024])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c629ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:16:43.858865Z",
     "iopub.status.busy": "2025-10-12T08:16:43.858466Z",
     "iopub.status.idle": "2025-10-12T08:16:43.863259Z",
     "shell.execute_reply": "2025-10-12T08:16:43.862567Z"
    },
    "papermill": {
     "duration": 0.010859,
     "end_time": "2025-10-12T08:16:43.864271",
     "exception": false,
     "start_time": "2025-10-12T08:16:43.853412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdcadbee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:16:43.874084Z",
     "iopub.status.busy": "2025-10-12T08:16:43.873886Z",
     "iopub.status.idle": "2025-10-12T08:16:58.226624Z",
     "shell.execute_reply": "2025-10-12T08:16:58.226014Z"
    },
    "papermill": {
     "duration": 14.359385,
     "end_time": "2025-10-12T08:16:58.228257",
     "exception": false,
     "start_time": "2025-10-12T08:16:43.868872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 08:16:45.546638: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760257005.773770      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760257005.842375      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    # Validate model size\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "\n",
    "    # Define paths\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    # Download files\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    for filename in filenames:\n",
    "        file_url = os.path.join(base_url, model_size, filename)\n",
    "        backup_url = os.path.join(backup_base_url, model_size, filename)\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        download_file(file_url, file_path, backup_url)\n",
    "\n",
    "    # Load settings and params\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "    return settings, params\n",
    "\n",
    "\n",
    "def download_file(url, destination, backup_url=None):\n",
    "    def _attempt_download(download_url):\n",
    "        with urllib.request.urlopen(download_url) as response:\n",
    "            # Get the total file size from headers, defaulting to 0 if not present\n",
    "            file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "            # Check if file exists and has the same size\n",
    "            if os.path.exists(destination):\n",
    "                file_size_local = os.path.getsize(destination)\n",
    "                if file_size == file_size_local:\n",
    "                    print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                    return True  # Indicate success without re-downloading\n",
    "\n",
    "            block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "            # Initialize the progress bar with total file size\n",
    "            progress_bar_description = os.path.basename(download_url)\n",
    "            with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
    "                with open(destination, \"wb\") as file:\n",
    "                    while True:\n",
    "                        chunk = response.read(block_size)\n",
    "                        if not chunk:\n",
    "                            break\n",
    "                        file.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "            return True\n",
    "\n",
    "    try:\n",
    "        if _attempt_download(url):\n",
    "            return\n",
    "    except (urllib.error.HTTPError, urllib.error.URLError):\n",
    "        if backup_url is not None:\n",
    "            print(f\"Primary URL ({url}) failed. Attempting backup URL: {backup_url}\")\n",
    "            try:\n",
    "                if _attempt_download(backup_url):\n",
    "                    return\n",
    "            except urllib.error.HTTPError:\n",
    "                pass\n",
    "\n",
    "        # If we reach here, both attempts have failed\n",
    "        error_message = (\n",
    "            f\"Failed to download from both primary URL ({url})\"\n",
    "            f\"{' and backup URL (' + backup_url + ')' if backup_url else ''}.\"\n",
    "            \"\\nCheck your internet connection or the file availability.\\n\"\n",
    "            \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\"\n",
    "        )\n",
    "        print(error_message)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Alternative way using `requests`\n",
    "\"\"\"\n",
    "def download_file(url, destination):\n",
    "    # Send a GET request to download the file in streaming mode\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Get the total file size from headers, defaulting to 0 if not present\n",
    "    file_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    # Check if file exists and has the same size\n",
    "    if os.path.exists(destination):\n",
    "        file_size_local = os.path.getsize(destination)\n",
    "        if file_size == file_size_local:\n",
    "            print(f\"File already exists and is up-to-date: {destination}\")\n",
    "            return\n",
    "\n",
    "    # Define the block size for reading the file\n",
    "    block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "    # Initialize the progress bar with total file size\n",
    "    progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
    "    with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
    "        # Open the destination file in binary write mode\n",
    "        with open(destination, \"wb\") as file:\n",
    "            # Iterate over the file data in chunks\n",
    "            for chunk in response.iter_content(block_size):\n",
    "                progress_bar.update(len(chunk))  # Update progress bar\n",
    "                file.write(chunk)  # Write the chunk to the file\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc108530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:16:58.239012Z",
     "iopub.status.busy": "2025-10-12T08:16:58.238542Z",
     "iopub.status.idle": "2025-10-12T08:16:58.271244Z",
     "shell.execute_reply": "2025-10-12T08:16:58.270569Z"
    },
    "papermill": {
     "duration": 0.039573,
     "end_time": "2025-10-12T08:16:58.272639",
     "exception": false,
     "start_time": "2025-10-12T08:16:58.233066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#####################################\n",
    "# Chapter 2\n",
    "#####################################\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 3\n",
    "#####################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 4\n",
    "#####################################\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 5\n",
    "#####################################\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7b1e69d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:16:58.283306Z",
     "iopub.status.busy": "2025-10-12T08:16:58.283063Z",
     "iopub.status.idle": "2025-10-12T08:17:20.110264Z",
     "shell.execute_reply": "2025-10-12T08:17:20.109370Z"
    },
    "papermill": {
     "duration": 21.834179,
     "end_time": "2025-10-12T08:17:20.111652",
     "exception": false,
     "start_time": "2025-10-12T08:16:58.277473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77.0/77.0 [00:00<00:00, 125kiB/s]\n",
      "encoder.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:00<00:00, 5.50MiB/s]\n",
      "hparams.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.0/90.0 [00:00<00:00, 175kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498M/498M [00:18<00:00, 27.2MiB/s]\n",
      "model.ckpt.index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.21k/5.21k [00:00<00:00, 10.4MiB/s]\n",
      "model.ckpt.meta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 471k/471k [00:00<00:00, 3.64MiB/s]\n",
      "vocab.bpe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 3.50MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3934d26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:17:20.136400Z",
     "iopub.status.busy": "2025-10-12T08:17:20.136155Z",
     "iopub.status.idle": "2025-10-12T08:17:21.549321Z",
     "shell.execute_reply": "2025-10-12T08:17:21.548300Z"
    },
    "papermill": {
     "duration": 1.426788,
     "end_time": "2025-10-12T08:17:21.550660",
     "exception": false,
     "start_time": "2025-10-12T08:17:20.123872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b0605a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:17:21.576122Z",
     "iopub.status.busy": "2025-10-12T08:17:21.575892Z",
     "iopub.status.idle": "2025-10-12T08:17:24.645025Z",
     "shell.execute_reply": "2025-10-12T08:17:24.644168Z"
    },
    "papermill": {
     "duration": 3.083699,
     "end_time": "2025-10-12T08:17:24.646355",
     "exception": false,
     "start_time": "2025-10-12T08:17:21.562656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "539c9d97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:17:24.671653Z",
     "iopub.status.busy": "2025-10-12T08:17:24.671401Z",
     "iopub.status.idle": "2025-10-12T08:17:24.676553Z",
     "shell.execute_reply": "2025-10-12T08:17:24.675625Z"
    },
    "papermill": {
     "duration": 0.018947,
     "end_time": "2025-10-12T08:17:24.677733",
     "exception": false,
     "start_time": "2025-10-12T08:17:24.658786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe75dcc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:17:24.702334Z",
     "iopub.status.busy": "2025-10-12T08:17:24.702101Z",
     "iopub.status.idle": "2025-10-12T08:17:24.705988Z",
     "shell.execute_reply": "2025-10-12T08:17:24.705274Z"
    },
    "papermill": {
     "duration": 0.017383,
     "end_time": "2025-10-12T08:17:24.707053",
     "exception": false,
     "start_time": "2025-10-12T08:17:24.689670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af2d3aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:17:24.733175Z",
     "iopub.status.busy": "2025-10-12T08:17:24.732276Z",
     "iopub.status.idle": "2025-10-12T08:17:24.737613Z",
     "shell.execute_reply": "2025-10-12T08:17:24.737034Z"
    },
    "papermill": {
     "duration": 0.019796,
     "end_time": "2025-10-12T08:17:24.738672",
     "exception": false,
     "start_time": "2025-10-12T08:17:24.718876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7dfc171",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:17:24.765138Z",
     "iopub.status.busy": "2025-10-12T08:17:24.764665Z",
     "iopub.status.idle": "2025-10-12T08:17:24.782999Z",
     "shell.execute_reply": "2025-10-12T08:17:24.782445Z"
    },
    "papermill": {
     "duration": 0.032242,
     "end_time": "2025-10-12T08:17:24.784096",
     "exception": false,
     "start_time": "2025-10-12T08:17:24.751854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Unfreeze the last 3 transformer blocks\n",
    "for block in model.trf_blocks[-8:]:\n",
    "    for param in block.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Unfreeze the final normalization layer\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07945c7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:17:24.808993Z",
     "iopub.status.busy": "2025-10-12T08:17:24.808806Z",
     "iopub.status.idle": "2025-10-12T08:17:24.814044Z",
     "shell.execute_reply": "2025-10-12T08:17:24.813243Z"
    },
    "papermill": {
     "duration": 0.01875,
     "end_time": "2025-10-12T08:17:24.815094",
     "exception": false,
     "start_time": "2025-10-12T08:17:24.796344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d499058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:17:24.839903Z",
     "iopub.status.busy": "2025-10-12T08:17:24.839648Z",
     "iopub.status.idle": "2025-10-12T08:17:24.923194Z",
     "shell.execute_reply": "2025-10-12T08:17:24.922346Z"
    },
    "papermill": {
     "duration": 0.097803,
     "end_time": "2025-10-12T08:17:24.924599",
     "exception": false,
     "start_time": "2025-10-12T08:17:24.826796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "087bc7e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:17:24.949871Z",
     "iopub.status.busy": "2025-10-12T08:17:24.949585Z",
     "iopub.status.idle": "2025-10-12T08:28:07.357647Z",
     "shell.execute_reply": "2025-10-12T08:28:07.356780Z"
    },
    "papermill": {
     "duration": 642.422297,
     "end_time": "2025-10-12T08:28:07.359065",
     "exception": false,
     "start_time": "2025-10-12T08:17:24.936768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n",
      "\n",
      "===== EPOCH 1/13 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2360 | Train ROC-AUC: 0.5253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6411 | Validation ROC-AUC: 0.8887\n",
      "ðŸ’¾ Saved model checkpoint for epoch 1\n",
      "âœ… New BEST model saved (ROC-AUC = 0.8887)\n",
      "\n",
      "===== EPOCH 2/13 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4827 | Train ROC-AUC: 0.8544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3346 | Validation ROC-AUC: 0.9272\n",
      "ðŸ’¾ Saved model checkpoint for epoch 2\n",
      "âœ… New BEST model saved (ROC-AUC = 0.9272)\n",
      "\n",
      "===== EPOCH 3/13 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3227 | Train ROC-AUC: 0.9250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3127 | Validation ROC-AUC: 0.9478\n",
      "ðŸ’¾ Saved model checkpoint for epoch 3\n",
      "âœ… New BEST model saved (ROC-AUC = 0.9478)\n",
      "\n",
      "===== EPOCH 4/13 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2271 | Train ROC-AUC: 0.9625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3236 | Validation ROC-AUC: 0.9588\n",
      "ðŸ’¾ Saved model checkpoint for epoch 4\n",
      "âœ… New BEST model saved (ROC-AUC = 0.9588)\n",
      "\n",
      "===== EPOCH 5/13 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1554 | Train ROC-AUC: 0.9849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3229 | Validation ROC-AUC: 0.9643\n",
      "ðŸ’¾ Saved model checkpoint for epoch 5\n",
      "âœ… New BEST model saved (ROC-AUC = 0.9643)\n",
      "\n",
      "===== EPOCH 6/13 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1186 | Train ROC-AUC: 0.9930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4815 | Validation ROC-AUC: 0.9121\n",
      "ðŸ’¾ Saved model checkpoint for epoch 6\n",
      "\n",
      "===== EPOCH 7/13 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0986 | Train ROC-AUC: 0.9947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3051 | Validation ROC-AUC: 0.9588\n",
      "ðŸ’¾ Saved model checkpoint for epoch 7\n",
      "\n",
      "===== EPOCH 8/13 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0347 | Train ROC-AUC: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6107 | Validation ROC-AUC: 0.9299\n",
      "ðŸ’¾ Saved model checkpoint for epoch 8\n",
      "\n",
      "===== EPOCH 9/13 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0333 | Train ROC-AUC: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4700 | Validation ROC-AUC: 0.9341\n",
      "ðŸ’¾ Saved model checkpoint for epoch 9\n",
      "\n",
      "===== EPOCH 10/13 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0366 | Train ROC-AUC: 0.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3863 | Validation ROC-AUC: 0.9492\n",
      "ðŸ’¾ Saved model checkpoint for epoch 10\n",
      "\n",
      "===== EPOCH 11/13 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0528 | Train ROC-AUC: 0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6548 | Validation ROC-AUC: 0.9505\n",
      "ðŸ’¾ Saved model checkpoint for epoch 11\n",
      "\n",
      "===== EPOCH 12/13 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0548 | Train ROC-AUC: 0.9957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1571 | Validation ROC-AUC: 0.9931\n",
      "ðŸ’¾ Saved model checkpoint for epoch 12\n",
      "âœ… New BEST model saved (ROC-AUC = 0.9931)\n",
      "\n",
      "===== EPOCH 13/13 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1339 | Train ROC-AUC: 0.9882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2814 | Validation ROC-AUC: 0.9670\n",
      "ðŸ’¾ Saved model checkpoint for epoch 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 13  # You can increase later\n",
    "best_auc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n===== EPOCH {epoch+1}/{num_epochs} =====\")\n",
    "    model.train()\n",
    "    train_losses, train_probs, train_labels = [], [], []\n",
    "\n",
    "    # --- Training ---\n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)                   # (batch, seq_len, 2)\n",
    "        logits = outputs[:, -1, :]                # last token\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss and predictions\n",
    "        probs = F.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()\n",
    "        train_probs.extend(probs)\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    # Compute ROC-AUC for training\n",
    "    train_auc = roc_auc_score(train_labels, train_probs)\n",
    "    print(f\"Train Loss: {sum(train_losses)/len(train_losses):.4f} | Train ROC-AUC: {train_auc:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_losses, val_probs, val_labels = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs[:, -1, :]\n",
    "\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            val_probs.extend(probs)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_auc = roc_auc_score(val_labels, val_probs)\n",
    "    val_loss = sum(val_losses) / len(val_losses)\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Validation ROC-AUC: {val_auc:.4f}\")\n",
    "\n",
    "    # --- Save model for this epoch ---\n",
    "    torch.save(model.state_dict(), f\"gpt2_epoch_{epoch+1}.pt\")\n",
    "    print(f\"ðŸ’¾ Saved model checkpoint for epoch {epoch+1}\")\n",
    "\n",
    "    # --- Save best model based on ROC-AUC ---\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        torch.save(model.state_dict(), f\"gpt2_best_model.pt\")\n",
    "        print(f\"âœ… New BEST model saved (ROC-AUC = {val_auc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65747697",
   "metadata": {
    "papermill": {
     "duration": 0.042347,
     "end_time": "2025-10-12T08:28:07.454402",
     "exception": false,
     "start_time": "2025-10-12T08:28:07.412055",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5219fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:28:07.522252Z",
     "iopub.status.busy": "2025-10-12T08:28:07.521271Z",
     "iopub.status.idle": "2025-10-12T08:28:32.129027Z",
     "shell.execute_reply": "2025-10-12T08:28:32.128059Z"
    },
    "papermill": {
     "duration": 24.643015,
     "end_time": "2025-10-12T08:28:32.130397",
     "exception": false,
     "start_time": "2025-10-12T08:28:07.487382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loading model from epoch 13: gpt2_epoch_13.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:24<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… submission.csv generated successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_cheating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>form_r_AAABmJpaBSZniRCnvHlBs66K</td>\n",
       "      <td>0.982143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>form_r_JiL6ylwnijP66wTCo52XSP</td>\n",
       "      <td>0.974475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>form_r_AAABmLN7KCDw89RJl3RORa5n</td>\n",
       "      <td>0.990610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>form_r_AAABmN2ij3UoaExF4jxFcZ_7</td>\n",
       "      <td>0.914473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>form_r_AAABmJYbGzEuL3gArMVHMZ1P</td>\n",
       "      <td>0.006426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  is_cheating\n",
       "0  form_r_AAABmJpaBSZniRCnvHlBs66K     0.982143\n",
       "1    form_r_JiL6ylwnijP66wTCo52XSP     0.974475\n",
       "2  form_r_AAABmLN7KCDw89RJl3RORa5n     0.990610\n",
       "3  form_r_AAABmN2ij3UoaExF4jxFcZ_7     0.914473\n",
       "4  form_r_AAABmJYbGzEuL3gArMVHMZ1P     0.006426"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================\n",
    "# Load test data\n",
    "# ==========================\n",
    "test_df = pd.read_csv(\"/kaggle/input/mercor-ai-detection/test.csv\")\n",
    "\n",
    "# Store IDs separately\n",
    "ids = test_df[\"id\"].values\n",
    "\n",
    "# Combine topic + answer into one text field\n",
    "test_df[\"text\"] = test_df[\"topic\"].astype(str) + \" \" + test_df[\"answer\"].astype(str)\n",
    "\n",
    "# ==========================\n",
    "# Tokenize test data\n",
    "# ==========================\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "max_length = 1024\n",
    "pad_token_id = 50256\n",
    "\n",
    "encoded_texts = [\n",
    "    tokenizer.encode(str(text)) for text in test_df[\"text\"]\n",
    "]\n",
    "\n",
    "# Truncate and pad sequences\n",
    "encoded_texts = [\n",
    "    (encoded[:max_length] + [pad_token_id] * max(0, max_length - len(encoded)))\n",
    "    for encoded in encoded_texts\n",
    "]\n",
    "\n",
    "# Convert to tensor\n",
    "test_tensors = torch.tensor(encoded_texts, dtype=torch.long)\n",
    "\n",
    "# ==========================\n",
    "# Load trained model (epoch 16)\n",
    "# ==========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "epoch_num = 13\n",
    "model_path = f\"gpt2_epoch_{epoch_num}.pt\"  # name your model files this way when saving\n",
    "\n",
    "print(f\"âœ… Loading model from epoch {epoch_num}: {model_path}\")\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ==========================\n",
    "# Predict probabilities\n",
    "# ==========================\n",
    "pred_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_tensors), 8), desc=\"Predicting\"):\n",
    "        batch_inputs = test_tensors[i:i+8].to(device)\n",
    "        outputs = model(batch_inputs)\n",
    "        logits = outputs[:, -1, :]  # final token logits\n",
    "        probs = F.softmax(logits, dim=1)[:, 1]  # probability of cheating\n",
    "        pred_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# ==========================\n",
    "# Prepare submission\n",
    "# ==========================\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": ids,\n",
    "    \"is_cheating\": pred_probs  # continuous probabilities (for ROC-AUC)\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"âœ… submission.csv generated successfully!\")\n",
    "display(submission.head())\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14006468,
     "sourceId": 117171,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 722.239193,
   "end_time": "2025-10-12T08:28:35.294185",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-12T08:16:33.054992",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
