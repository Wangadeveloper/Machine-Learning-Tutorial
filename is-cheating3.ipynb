{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23d1b5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:52:02.679302Z",
     "iopub.status.busy": "2025-10-10T06:52:02.678666Z",
     "iopub.status.idle": "2025-10-10T06:52:04.006813Z",
     "shell.execute_reply": "2025-10-10T06:52:04.006014Z"
    },
    "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
    "outputId": "9495f150-9d79-4910-d6e7-6c0d9aae4a41",
    "papermill": {
     "duration": 1.335283,
     "end_time": "2025-10-10T06:52:04.008208",
     "exception": false,
     "start_time": "2025-10-10T06:52:02.672925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mercor-ai-detection/sample_submission.csv\n",
      "/kaggle/input/mercor-ai-detection/train.csv\n",
      "/kaggle/input/mercor-ai-detection/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750a2893",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:52:04.021885Z",
     "iopub.status.busy": "2025-10-10T06:52:04.021559Z",
     "iopub.status.idle": "2025-10-10T06:52:04.088080Z",
     "shell.execute_reply": "2025-10-10T06:52:04.087161Z"
    },
    "papermill": {
     "duration": 0.075374,
     "end_time": "2025-10-10T06:52:04.089360",
     "exception": false,
     "start_time": "2025-10-10T06:52:04.013986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merging:\n",
      "                                id  \\\n",
      "0    form_r_d5TpupthGXvfy58LDIbkqp   \n",
      "1  form_r_AAABmK2rZBBbtfdHxzhBY4C4   \n",
      "2    form_r_Cz6IJIWUj1B7pdyO6zPhGF   \n",
      "3  form_r_AAABmKyCfdO6NLv8eqBPCYD_   \n",
      "4    form_r_WS8I1NJpwvIcFL3Xv0Qhuc   \n",
      "\n",
      "                                               topic  \\\n",
      "0  A girl wakes from a dream and she is not sure ...   \n",
      "1  A journalistic review piece about the top 6 ai...   \n",
      "2  The influence of fictional universities in cam...   \n",
      "3                           Why do girls love horses   \n",
      "4  Every year, a remote mountain town elects a ne...   \n",
      "\n",
      "                                              answer  is_cheating  \n",
      "0  My eyes flew open, and the air around me feels...            1  \n",
      "1  Robot Butlers in the year of 2025. What are th...            0  \n",
      "2  In recent years, apparel featuring the names a...            1  \n",
      "3  The moment before I hit the dirt, I thought we...            0  \n",
      "4  In the valley of Eldermist, were the mountains...            1  \n",
      "After merging:\n",
      "                                                text  is_cheating\n",
      "0  A girl wakes from a dream and she is not sure ...            1\n",
      "1  A journalistic review piece about the top 6 ai...            0\n",
      "2  The influence of fictional universities in cam...            1\n",
      "3  Why do girls love horses The moment before I h...            0\n",
      "4  Every year, a remote mountain town elects a ne...            1\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ðŸ§¹ STEP 1: Load and preprocess dataset for AI-text detection\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "\n",
    "# Adjust these paths as needed for your Kaggle environment\n",
    "train_path = \"/kaggle/input/mercor-ai-detection/train.csv\"\n",
    "test_path = \"/kaggle/input/mercor-ai-detection/test.csv\"\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Before merging:\")\n",
    "print(train.head())\n",
    "\n",
    "# Combine topic and answer into a single text column\n",
    "train[\"text\"] = train[\"topic\"].astype(str) + \" \" + train[\"answer\"].astype(str)\n",
    "test[\"text\"] = test[\"topic\"].astype(str) + \" \" + test[\"answer\"].astype(str)\n",
    "\n",
    "# Keep only necessary columns\n",
    "train = train[[\"text\", \"is_cheating\"]]\n",
    "test = test[[\"id\", \"text\"]]\n",
    "\n",
    "print(\"After merging:\")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eac6be1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:52:04.099685Z",
     "iopub.status.busy": "2025-10-10T06:52:04.099229Z",
     "iopub.status.idle": "2025-10-10T06:52:04.107157Z",
     "shell.execute_reply": "2025-10-10T06:52:04.106492Z"
    },
    "papermill": {
     "duration": 0.014519,
     "end_time": "2025-10-10T06:52:04.108151",
     "exception": false,
     "start_time": "2025-10-10T06:52:04.093632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_cheating\n",
      "1    147\n",
      "0    122\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train[\"is_cheating\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3cf100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:52:04.118206Z",
     "iopub.status.busy": "2025-10-10T06:52:04.117553Z",
     "iopub.status.idle": "2025-10-10T06:52:04.150811Z",
     "shell.execute_reply": "2025-10-10T06:52:04.149976Z"
    },
    "papermill": {
     "duration": 0.039848,
     "end_time": "2025-10-10T06:52:04.152176",
     "exception": false,
     "start_time": "2025-10-10T06:52:04.112328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 215\n",
      "Validation samples: 54\n"
     ]
    }
   ],
   "source": [
    "def random_split(df, train_frac=0.8):\n",
    "    \"\"\"\n",
    "    Randomly splits a DataFrame into training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataset.\n",
    "        train_frac (float): Fraction of data to use for training (default 0.8).\n",
    "        \n",
    "    Returns:\n",
    "        train_df (pd.DataFrame), validation_df (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    # Shuffle the DataFrame\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Compute split index\n",
    "    train_end = int(len(df) * train_frac)\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:]\n",
    "\n",
    "    return train_df, validation_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "train_df, validation_df = random_split(train, train_frac=0.8)\n",
    "\n",
    "# Optionally save to disk\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "validation_df.to_csv(\"validation.csv\", index=False)\n",
    "test.to_csv(\"test.csv\" ,index=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(validation_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02493e36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:52:04.164212Z",
     "iopub.status.busy": "2025-10-10T06:52:04.163573Z",
     "iopub.status.idle": "2025-10-10T06:52:05.234722Z",
     "shell.execute_reply": "2025-10-10T06:52:05.233812Z"
    },
    "papermill": {
     "duration": 1.077272,
     "end_time": "2025-10-10T06:52:05.236013",
     "exception": false,
     "start_time": "2025-10-10T06:52:04.158741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af38afe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:52:05.245428Z",
     "iopub.status.busy": "2025-10-10T06:52:05.245208Z",
     "iopub.status.idle": "2025-10-10T06:52:08.930061Z",
     "shell.execute_reply": "2025-10-10T06:52:08.929227Z"
    },
    "papermill": {
     "duration": 3.690867,
     "end_time": "2025-10-10T06:52:08.931298",
     "exception": false,
     "start_time": "2025-10-10T06:52:05.240431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token length (train set): 1024\n",
      "Train samples: 215 | Validation samples: 54\n",
      "Sample token IDs: tensor([ 9704, 13629,   287,   262,  9552,   995, 22137,   287,   262,  9552,\n",
      "          995,   318,   257,   845,  3024,  7243,   287,   284, 12545,   640])\n",
      "Sample label: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "class AIDetectionDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer=None, max_length=1024, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Initialize tokenizer (use GPT-2 encoding if none provided)\n",
    "        self.tokenizer = tokenizer or tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "        # Pre-tokenize all texts\n",
    "        self.encoded_texts = [\n",
    "            self.tokenizer.encode(str(text)) for text in self.data[\"text\"]\n",
    "        ]\n",
    "\n",
    "        # Determine or set max_length\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "\n",
    "        # Truncate and pad sequences\n",
    "        self.encoded_texts = [\n",
    "            (encoded[:self.max_length] + [pad_token_id] * max(0, self.max_length - len(encoded)))\n",
    "            for encoded in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"is_cheating\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        \"\"\"Find the maximum tokenized sequence length.\"\"\"\n",
    "        return max(len(encoded) for encoded in self.encoded_texts)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "train_dataset = AIDetectionDataset(\"train.csv\", tokenizer=tokenizer)\n",
    "val_dataset = AIDetectionDataset(\"validation.csv\", tokenizer=tokenizer)\n",
    "test_dataset = AIDetectionDataset(\"test.csv\", tokenizer=tokenizer)\n",
    "\n",
    "print(f\"Max token length (train set): {train_dataset.max_length}\")\n",
    "print(f\"Train samples: {len(train_dataset)} | Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Example: get one batch item\n",
    "sample_input, sample_label = train_dataset[0]\n",
    "print(\"Sample token IDs:\", sample_input[:20])\n",
    "print(\"Sample label:\", sample_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2241dc17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:52:08.941699Z",
     "iopub.status.busy": "2025-10-10T06:52:08.941111Z",
     "iopub.status.idle": "2025-10-10T06:52:08.950493Z",
     "shell.execute_reply": "2025-10-10T06:52:08.949871Z"
    },
    "papermill": {
     "duration": 0.015717,
     "end_time": "2025-10-10T06:52:08.951525",
     "exception": false,
     "start_time": "2025-10-10T06:52:08.935808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 26 | Val batches: 7 | Test batches: 33\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)} | Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea4f7e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:52:08.960931Z",
     "iopub.status.busy": "2025-10-10T06:52:08.960442Z",
     "iopub.status.idle": "2025-10-10T06:52:09.012967Z",
     "shell.execute_reply": "2025-10-10T06:52:09.012267Z"
    },
    "papermill": {
     "duration": 0.058381,
     "end_time": "2025-10-10T06:52:09.014160",
     "exception": false,
     "start_time": "2025-10-10T06:52:08.955779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 1024])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a609981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:52:09.023515Z",
     "iopub.status.busy": "2025-10-10T06:52:09.023322Z",
     "iopub.status.idle": "2025-10-10T06:52:09.027647Z",
     "shell.execute_reply": "2025-10-10T06:52:09.027092Z"
    },
    "papermill": {
     "duration": 0.010039,
     "end_time": "2025-10-10T06:52:09.028635",
     "exception": false,
     "start_time": "2025-10-10T06:52:09.018596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28fc01a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:52:09.037926Z",
     "iopub.status.busy": "2025-10-10T06:52:09.037735Z",
     "iopub.status.idle": "2025-10-10T06:52:21.935939Z",
     "shell.execute_reply": "2025-10-10T06:52:21.935343Z"
    },
    "papermill": {
     "duration": 12.904555,
     "end_time": "2025-10-10T06:52:21.937392",
     "exception": false,
     "start_time": "2025-10-10T06:52:09.032837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 06:52:10.434297: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760079130.620192      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760079130.674622      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    # Validate model size\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "\n",
    "    # Define paths\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    # Download files\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    for filename in filenames:\n",
    "        file_url = os.path.join(base_url, model_size, filename)\n",
    "        backup_url = os.path.join(backup_base_url, model_size, filename)\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        download_file(file_url, file_path, backup_url)\n",
    "\n",
    "    # Load settings and params\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "    return settings, params\n",
    "\n",
    "\n",
    "def download_file(url, destination, backup_url=None):\n",
    "    def _attempt_download(download_url):\n",
    "        with urllib.request.urlopen(download_url) as response:\n",
    "            # Get the total file size from headers, defaulting to 0 if not present\n",
    "            file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "            # Check if file exists and has the same size\n",
    "            if os.path.exists(destination):\n",
    "                file_size_local = os.path.getsize(destination)\n",
    "                if file_size == file_size_local:\n",
    "                    print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                    return True  # Indicate success without re-downloading\n",
    "\n",
    "            block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "            # Initialize the progress bar with total file size\n",
    "            progress_bar_description = os.path.basename(download_url)\n",
    "            with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
    "                with open(destination, \"wb\") as file:\n",
    "                    while True:\n",
    "                        chunk = response.read(block_size)\n",
    "                        if not chunk:\n",
    "                            break\n",
    "                        file.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "            return True\n",
    "\n",
    "    try:\n",
    "        if _attempt_download(url):\n",
    "            return\n",
    "    except (urllib.error.HTTPError, urllib.error.URLError):\n",
    "        if backup_url is not None:\n",
    "            print(f\"Primary URL ({url}) failed. Attempting backup URL: {backup_url}\")\n",
    "            try:\n",
    "                if _attempt_download(backup_url):\n",
    "                    return\n",
    "            except urllib.error.HTTPError:\n",
    "                pass\n",
    "\n",
    "        # If we reach here, both attempts have failed\n",
    "        error_message = (\n",
    "            f\"Failed to download from both primary URL ({url})\"\n",
    "            f\"{' and backup URL (' + backup_url + ')' if backup_url else ''}.\"\n",
    "            \"\\nCheck your internet connection or the file availability.\\n\"\n",
    "            \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\"\n",
    "        )\n",
    "        print(error_message)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Alternative way using `requests`\n",
    "\"\"\"\n",
    "def download_file(url, destination):\n",
    "    # Send a GET request to download the file in streaming mode\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Get the total file size from headers, defaulting to 0 if not present\n",
    "    file_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    # Check if file exists and has the same size\n",
    "    if os.path.exists(destination):\n",
    "        file_size_local = os.path.getsize(destination)\n",
    "        if file_size == file_size_local:\n",
    "            print(f\"File already exists and is up-to-date: {destination}\")\n",
    "            return\n",
    "\n",
    "    # Define the block size for reading the file\n",
    "    block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "    # Initialize the progress bar with total file size\n",
    "    progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
    "    with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
    "        # Open the destination file in binary write mode\n",
    "        with open(destination, \"wb\") as file:\n",
    "            # Iterate over the file data in chunks\n",
    "            for chunk in response.iter_content(block_size):\n",
    "                progress_bar.update(len(chunk))  # Update progress bar\n",
    "                file.write(chunk)  # Write the chunk to the file\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1803543",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:52:21.947733Z",
     "iopub.status.busy": "2025-10-10T06:52:21.947361Z",
     "iopub.status.idle": "2025-10-10T06:52:21.978383Z",
     "shell.execute_reply": "2025-10-10T06:52:21.977681Z"
    },
    "papermill": {
     "duration": 0.037402,
     "end_time": "2025-10-10T06:52:21.979464",
     "exception": false,
     "start_time": "2025-10-10T06:52:21.942062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#####################################\n",
    "# Chapter 2\n",
    "#####################################\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 3\n",
    "#####################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 4\n",
    "#####################################\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 5\n",
    "#####################################\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b073d2f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:52:21.988880Z",
     "iopub.status.busy": "2025-10-10T06:52:21.988489Z",
     "iopub.status.idle": "2025-10-10T06:53:27.713407Z",
     "shell.execute_reply": "2025-10-10T06:53:27.712769Z"
    },
    "papermill": {
     "duration": 65.731098,
     "end_time": "2025-10-10T06:53:27.714788",
     "exception": false,
     "start_time": "2025-10-10T06:52:21.983690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77.0/77.0 [00:00<00:00, 156kiB/s]\n",
      "encoder.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:00<00:00, 5.10MiB/s]\n",
      "hparams.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.0/90.0 [00:00<00:00, 177kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498M/498M [01:02<00:00, 8.00MiB/s]\n",
      "model.ckpt.index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.21k/5.21k [00:00<00:00, 9.24MiB/s]\n",
      "model.ckpt.meta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 471k/471k [00:00<00:00, 3.15MiB/s]\n",
      "vocab.bpe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 4.13MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54ac9a94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:53:27.768698Z",
     "iopub.status.busy": "2025-10-10T06:53:27.768500Z",
     "iopub.status.idle": "2025-10-10T06:53:29.078458Z",
     "shell.execute_reply": "2025-10-10T06:53:29.077569Z"
    },
    "papermill": {
     "duration": 1.338017,
     "end_time": "2025-10-10T06:53:29.079812",
     "exception": false,
     "start_time": "2025-10-10T06:53:27.741795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76a6f802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:53:29.136677Z",
     "iopub.status.busy": "2025-10-10T06:53:29.136454Z",
     "iopub.status.idle": "2025-10-10T06:53:32.112687Z",
     "shell.execute_reply": "2025-10-10T06:53:32.111863Z"
    },
    "papermill": {
     "duration": 3.004612,
     "end_time": "2025-10-10T06:53:32.114108",
     "exception": false,
     "start_time": "2025-10-10T06:53:29.109496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8d6c9dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:53:32.168471Z",
     "iopub.status.busy": "2025-10-10T06:53:32.168044Z",
     "iopub.status.idle": "2025-10-10T06:53:32.172642Z",
     "shell.execute_reply": "2025-10-10T06:53:32.171985Z"
    },
    "papermill": {
     "duration": 0.033237,
     "end_time": "2025-10-10T06:53:32.174355",
     "exception": false,
     "start_time": "2025-10-10T06:53:32.141118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d78a1379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:53:32.229838Z",
     "iopub.status.busy": "2025-10-10T06:53:32.229421Z",
     "iopub.status.idle": "2025-10-10T06:53:32.233137Z",
     "shell.execute_reply": "2025-10-10T06:53:32.232632Z"
    },
    "papermill": {
     "duration": 0.031798,
     "end_time": "2025-10-10T06:53:32.234154",
     "exception": false,
     "start_time": "2025-10-10T06:53:32.202356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52b3a6c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:53:32.289951Z",
     "iopub.status.busy": "2025-10-10T06:53:32.289771Z",
     "iopub.status.idle": "2025-10-10T06:53:32.294467Z",
     "shell.execute_reply": "2025-10-10T06:53:32.293796Z"
    },
    "papermill": {
     "duration": 0.033819,
     "end_time": "2025-10-10T06:53:32.295534",
     "exception": false,
     "start_time": "2025-10-10T06:53:32.261715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d95129ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:53:32.348957Z",
     "iopub.status.busy": "2025-10-10T06:53:32.348752Z",
     "iopub.status.idle": "2025-10-10T06:53:32.366460Z",
     "shell.execute_reply": "2025-10-10T06:53:32.365950Z"
    },
    "papermill": {
     "duration": 0.045543,
     "end_time": "2025-10-10T06:53:32.367490",
     "exception": false,
     "start_time": "2025-10-10T06:53:32.321947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Unfreeze the last 3 transformer blocks\n",
    "for block in model.trf_blocks[-4:]:\n",
    "    for param in block.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Unfreeze the final normalization layer\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "384bbd38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:53:32.421713Z",
     "iopub.status.busy": "2025-10-10T06:53:32.421140Z",
     "iopub.status.idle": "2025-10-10T06:53:32.425790Z",
     "shell.execute_reply": "2025-10-10T06:53:32.425183Z"
    },
    "papermill": {
     "duration": 0.032538,
     "end_time": "2025-10-10T06:53:32.426709",
     "exception": false,
     "start_time": "2025-10-10T06:53:32.394171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "297d6e44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:53:32.480604Z",
     "iopub.status.busy": "2025-10-10T06:53:32.480427Z",
     "iopub.status.idle": "2025-10-10T06:53:32.550947Z",
     "shell.execute_reply": "2025-10-10T06:53:32.550180Z"
    },
    "papermill": {
     "duration": 0.098905,
     "end_time": "2025-10-10T06:53:32.552051",
     "exception": false,
     "start_time": "2025-10-10T06:53:32.453146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0820fd8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:53:32.606398Z",
     "iopub.status.busy": "2025-10-10T06:53:32.606065Z",
     "iopub.status.idle": "2025-10-10T06:53:32.610689Z",
     "shell.execute_reply": "2025-10-10T06:53:32.609953Z"
    },
    "papermill": {
     "duration": 0.033076,
     "end_time": "2025-10-10T06:53:32.611687",
     "exception": false,
     "start_time": "2025-10-10T06:53:32.578611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "025b9b43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T06:53:32.666292Z",
     "iopub.status.busy": "2025-10-10T06:53:32.666091Z",
     "iopub.status.idle": "2025-10-10T07:04:49.364979Z",
     "shell.execute_reply": "2025-10-10T07:04:49.364034Z"
    },
    "papermill": {
     "duration": 676.727385,
     "end_time": "2025-10-10T07:04:49.366050",
     "exception": false,
     "start_time": "2025-10-10T06:53:32.638665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== EPOCH 1/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1244 | Train ROC-AUC: 0.4637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6893 | Validation ROC-AUC: 0.4821\n",
      "âœ… Saved new best model with ROC-AUC 0.4821\n",
      "\n",
      "===== EPOCH 2/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6721 | Train ROC-AUC: 0.6569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6713 | Validation ROC-AUC: 0.7830\n",
      "âœ… Saved new best model with ROC-AUC 0.7830\n",
      "\n",
      "===== EPOCH 3/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5867 | Train ROC-AUC: 0.7252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5520 | Validation ROC-AUC: 0.8558\n",
      "âœ… Saved new best model with ROC-AUC 0.8558\n",
      "\n",
      "===== EPOCH 4/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5634 | Train ROC-AUC: 0.7477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4459 | Validation ROC-AUC: 0.9258\n",
      "âœ… Saved new best model with ROC-AUC 0.9258\n",
      "\n",
      "===== EPOCH 5/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4507 | Train ROC-AUC: 0.8524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3654 | Validation ROC-AUC: 0.9780\n",
      "âœ… Saved new best model with ROC-AUC 0.9780\n",
      "\n",
      "===== EPOCH 6/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4565 | Train ROC-AUC: 0.8533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2999 | Validation ROC-AUC: 0.9780\n",
      "\n",
      "===== EPOCH 7/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3439 | Train ROC-AUC: 0.9210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1944 | Validation ROC-AUC: 0.9931\n",
      "âœ… Saved new best model with ROC-AUC 0.9931\n",
      "\n",
      "===== EPOCH 8/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2098 | Train ROC-AUC: 0.9789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1810 | Validation ROC-AUC: 0.9849\n",
      "\n",
      "===== EPOCH 9/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1055 | Train ROC-AUC: 0.9946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0842 | Validation ROC-AUC: 0.9986\n",
      "âœ… Saved new best model with ROC-AUC 0.9986\n",
      "\n",
      "===== EPOCH 10/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0746 | Train ROC-AUC: 0.9976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1222 | Validation ROC-AUC: 0.9918\n",
      "\n",
      "===== EPOCH 11/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0472 | Train ROC-AUC: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1248 | Validation ROC-AUC: 0.9945\n",
      "\n",
      "===== EPOCH 12/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0347 | Train ROC-AUC: 0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0955 | Validation ROC-AUC: 0.9945\n",
      "\n",
      "===== EPOCH 13/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0264 | Train ROC-AUC: 0.9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1071 | Validation ROC-AUC: 0.9918\n",
      "\n",
      "===== EPOCH 14/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0198 | Train ROC-AUC: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1758 | Validation ROC-AUC: 0.9973\n",
      "\n",
      "===== EPOCH 15/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0393 | Train ROC-AUC: 0.9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2676 | Validation ROC-AUC: 0.9725\n",
      "\n",
      "===== EPOCH 16/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0523 | Train ROC-AUC: 0.9989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0764 | Validation ROC-AUC: 1.0000\n",
      "âœ… Saved new best model with ROC-AUC 1.0000\n",
      "\n",
      "===== EPOCH 17/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0254 | Train ROC-AUC: 0.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1313 | Validation ROC-AUC: 0.9918\n",
      "\n",
      "===== EPOCH 18/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0113 | Train ROC-AUC: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2489 | Validation ROC-AUC: 0.9904\n",
      "\n",
      "===== EPOCH 19/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0051 | Train ROC-AUC: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1052 | Validation ROC-AUC: 0.9986\n",
      "\n",
      "===== EPOCH 20/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0024 | Train ROC-AUC: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0737 | Validation ROC-AUC: 0.9973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 20 # you can increase later\n",
    "best_auc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n===== EPOCH {epoch+1}/{num_epochs} =====\")\n",
    "    model.train()\n",
    "    train_losses, train_probs, train_labels = [], [], []\n",
    "\n",
    "    # --- Training ---\n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)                   # (batch, seq_len, 2)\n",
    "        logits = outputs[:, -1, :]                # last token\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss and predictions\n",
    "        probs = F.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()\n",
    "        train_probs.extend(probs)\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    # Compute ROC-AUC for training\n",
    "    train_auc = roc_auc_score(train_labels, train_probs)\n",
    "    print(f\"Train Loss: {sum(train_losses)/len(train_losses):.4f} | Train ROC-AUC: {train_auc:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_losses, val_probs, val_labels = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs[:, -1, :]\n",
    "\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            val_probs.extend(probs)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_auc = roc_auc_score(val_labels, val_probs)\n",
    "    val_loss = sum(val_losses) / len(val_losses)\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Validation ROC-AUC: {val_auc:.4f}\")\n",
    "\n",
    "    # --- Checkpoint if improved ---\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        torch.save(model.state_dict(), f\"best_gpt2_model_auc{val_auc:.4f}.pt\")\n",
    "        print(f\"âœ… Saved new best model with ROC-AUC {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "336a50a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T07:04:49.487995Z",
     "iopub.status.busy": "2025-10-10T07:04:49.487503Z",
     "iopub.status.idle": "2025-10-10T07:05:12.266030Z",
     "shell.execute_reply": "2025-10-10T07:05:12.265147Z"
    },
    "papermill": {
     "duration": 22.840036,
     "end_time": "2025-10-10T07:05:12.267243",
     "exception": false,
     "start_time": "2025-10-10T07:04:49.427207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loading model: best_gpt2_model_auc1.0000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:22<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… submission.csv generated successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_cheating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>form_r_AAABmJpaBSZniRCnvHlBs66K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>form_r_JiL6ylwnijP66wTCo52XSP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>form_r_AAABmLN7KCDw89RJl3RORa5n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>form_r_AAABmN2ij3UoaExF4jxFcZ_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>form_r_AAABmJYbGzEuL3gArMVHMZ1P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  is_cheating\n",
       "0  form_r_AAABmJpaBSZniRCnvHlBs66K            0\n",
       "1    form_r_JiL6ylwnijP66wTCo52XSP            1\n",
       "2  form_r_AAABmLN7KCDw89RJl3RORa5n            1\n",
       "3  form_r_AAABmN2ij3UoaExF4jxFcZ_7            0\n",
       "4  form_r_AAABmJYbGzEuL3gArMVHMZ1P            1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# ==========================\n",
    "# Load and prepare test data\n",
    "# ==========================\n",
    "test_df = pd.read_csv(\"/kaggle/input/mercor-ai-detection/test.csv\")\n",
    "\n",
    "# Store IDs separately\n",
    "ids = test_df[\"id\"].values\n",
    "\n",
    "# Combine text columns\n",
    "test_df[\"text\"] = test_df[\"topic\"].astype(str) + \" \" + test_df[\"answer\"].astype(str)\n",
    "\n",
    "# ==========================\n",
    "# Tokenize test data\n",
    "# ==========================\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "max_length = 1024\n",
    "pad_token_id = 50256\n",
    "\n",
    "encoded_texts = [\n",
    "    tokenizer.encode(str(text)) for text in test_df[\"text\"]\n",
    "]\n",
    "\n",
    "# Truncate and pad\n",
    "encoded_texts = [\n",
    "    (encoded[:max_length] + [pad_token_id] * max(0, max_length - len(encoded)))\n",
    "    for encoded in encoded_texts\n",
    "]\n",
    "\n",
    "# Convert to tensor\n",
    "test_tensors = torch.tensor(encoded_texts, dtype=torch.long)\n",
    "\n",
    "# ==========================\n",
    "# Predict using trained model\n",
    "# ==========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load best available model automatically\n",
    "model_paths = glob.glob(\"best_gpt2_model_auc*.pt\")\n",
    "if not model_paths:\n",
    "    raise FileNotFoundError(\"No trained model found in the current directory!\")\n",
    "best_model_path = sorted(model_paths)[-1]\n",
    "print(f\"âœ… Loading model: {best_model_path}\")\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_tensors), 8), desc=\"Predicting\"):\n",
    "        batch_inputs = test_tensors[i:i+8].to(device)\n",
    "        outputs = model(batch_inputs)\n",
    "        logits = outputs[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=1)[:, 1]\n",
    "        preds = (probs > 0.5).long().cpu().numpy()\n",
    "        pred_labels.extend(preds)\n",
    "\n",
    "# ==========================\n",
    "# Create submission\n",
    "# ==========================\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": ids,\n",
    "    \"is_cheating\": pred_labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"âœ… submission.csv generated successfully!\")\n",
    "display(submission.head())\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14006468,
     "sourceId": 117171,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 796.155906,
   "end_time": "2025-10-10T07:05:15.333990",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-10T06:51:59.178084",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
